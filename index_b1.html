<!DOCTYPE html>
<html>
<head>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css" integrity="sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf" crossorigin="anonymous">
  <link rel="stylesheet" href="styles.css" type="text/css" >
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

  <title> Implementing the Kalman Filter for adaptive autoregression </title>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
  <!-- <script type="text/javascript" src="latexmath.js"></script> -->

</head>

<body>
<main>


<div class="site-header"> 
  
  <div class="header-cont" style="font-family: 'Didact Gothic'; font-size: 20px; margin-right: 25px;"> 
    <b style="white-space: pre;">Implementing the Kalman Filter
for adaptive autoregression</b>
  </div>

  <div class="header-cont">   
    <!---->


  <div>
    <a href="https://sanderpaekivi.github.io/index.html" target="_blank" style="text-decoration: none;">
      <b style="color: white; font-family: 'Open Sans'; font-size: 30px; "> Home </b> 
    </a> 
  </div>
  


    <!---->
  </div>

</div>

<div class="content" style="margin-top: 0 ; margin-bottom: 0 ;">

      <h3>Introduction:</h3>
      <p style="text-align:justify; "> 

        If you found yourself on this page, I assume you've developed an interest in applying the promising Kalman Filter algorithm for regression tasks, hoping to perhaps deal with 
        issues of non-stationarity or simply obtain a higher time resolution for your results. What ever the specifics, I've been down that road and decided to write up some steps 
        that I've found useful in understanding the algorithm and my basic implementation. 

        <br><br>

        This blog post is a summary of what I've learned in my attempts to wrangle the Kalman Filter for the purpose of adaptive vector-autoregression - in practice the extraction of autoregression coefficients 
        in as continuous time as possible, between as many time-series as necessary. My motivation for looking into this implementation was application to statistical causality analysis, 
        where issues of non-stationarity can really obfuscate real relationships between time-series. A Kalman Filter based adaptive method was proposed in a variety of scientific
        papers, however the rigor in theoretical overview was oft lacking and contained errors, as well as the implementation code kept private. In principle, the method could be very powerful, 
        in practice I have (so far) found that easier solutions work quicker and as effectively though. 
        
        <br> <br>

        I currently don't regard this method as highly as I once did, and have not completely worked out all the nuances involved. There are many complexities involved and my practical 
        experience has proven simpler methods to be more approachable and by extension more productive. Nevertheless I decided to write up this summary in hopes that it can at leasts 
        be of assistance to others pursuing the same goal. 
        
        <br><br>
        
        The post is structured as follows: 
        
        <ol>
          <li>A basic derivation of Ordinary Least Squares (OLS) for regression</li>
          <li>A basic derivation of Recursive Least Squares (RLS) for regression </li>
          <li>A basic derivation of the discrete Kalman Filter how to adapt it for regression </li>
          <li>A rudimentary implementation of the above in Python</li>
          <li>Closing remarks, relevant citations and link to GitHub with other related code samples</li>
        </ol>  

      </p>  

      <h3>Autoregressive modelling:</h3>
      <p style="text-align:justify; "> 
        
        To start off, lets briefly cover step-by-step the conventional autoregressive (AR) process - it is a model for a time-series process, which is dependent upon it's own past 
        values - hence the prefix 'auto'. Often people also speak about vector-autoregression (VAR), which is the case of multiple time-series interacting with each other and 
        themselves. The vector part here will become clear soon, if it is not yet so (it's to do with the mathematical representation). For a single time-series, a simple AR process 
        is then the regression model between dependent \( y(t) \) and independend variables \( y(t-\tau) \), with \( \tau \) being the lag or time-delay. How far this lagged 
        influence goes is referred to as the order of the model, \(p\) and a model is written generally as AR(\(p\)) or VAR(\(p\)). 

        <br><br>

        Mathematically we write the example AR(\(p\)) process as:
        <!-- view-source:http://latex.userpage.fu-berlin.de/math.html for latex in html!!! -->

        \[
          y \left( t \right) = \sum_{\tau=1}^{p} c_\tau \ y \left( t-\tau \right) + \epsilon(t)
        \]
  
        Here we read it as modelling the value of the process \( y(t) \) at time \( t \) by summing over it's past values to a lag of \( \tau \), while multiplying each of these 
        past values by a coefficient \( c_\tau \). In this manner, the coefficients describe the influence of a past value, for all values of \( t \)! They cannot change, once the model
        has been fit. This is alright, if the underlying properties of the process do not change in this time frame. In any case however, we never expect the model coefficients to be
        able to match the lefthand side of the equation 100% due to noize in the data or errors in data measurement, thus this part is modelled with the \(\epsilon(t)\) term. In practice
        this is simply the residual of the modeled value (righthand side summation) subtracted from the actual value. 

        <br><br>

        To find these coefficients \( c_\tau \), we need to solve the above equation with the constraint that the \(\epsilon(t)\) term is as small as possible, minimizing the error. 
        To do this, we'll make use of matrix notation, to enable us fast extension to multiple time-series cases and also... it's just simpler like this. The above equation can be equally
        restated as:

        \[
        \overbrace{Y}^\text{N x 1}  = \overbrace{X}^\text{N x \(p\)} \times \overbrace{A}^\text{\(p\) x 1} + \overbrace{E}^\text{N x 1} 
        \]

        We now have the lagged value matrix capital X, which contains N rows of \(p\) columns, where each row represents the \(p\) lagged values of a particular \( y(t) \) where 
        \( t \in(1,2,3,...,N) \) and
        a matrix of regression coefficients, \( A \), which contains \(p\) rows and one column - there are only one set of these coefficients for all time points. A matrix multiplication 
        between these two yields a sequence of equations that make up the summation based formula, for each time step \( t \). Now we express the error term, specifically its 'norm' - 
        square absolute error, and minimize this criteria (there are other ways to fit a regression model, but this is know nas the Ordinary Least Squares or OLS method, which is most common).
        After some matrix algebra (it's relatively simple to follow along here, but not necessary), and get the following expression:

        \[
        || E ||^2 = ||Y - XA||^2 = (Y-XA)^T (Y-XA) = YY^T A^T X^T Y - Y^T X A + A^T X^T X^T A
        \]

        We now take it's partial derivative of the error with respect to the A matrix (coefficients), set it to zero (thus the solution we obtain is at a stationary point for the 
        target variable), and get the formula:

        \[
        A = (X^T X)^{-1} X^T Y
        \]

        What this means is, that given that we know what are both \( Y \) and \( X \) (from the real data, they are just a combination of values at \( t \) and it's lags), to get 
        regression coefficients that provide a minimum value for the error term \( \epsilon \) - thus giving as accurate a translation from past values to future ones! 

      </p>  

      <h3>Recursive Autoregressive modelling:</h3>
      <p style="text-align:justify; "> 

        The next useful (side)step (in my humble opinion) in understanding the Kalman Filter is the Recursive AR process! 
        Consider this - what if we fit an AR model, and then got more data. We could just re-evaluate the model, but perhaps this is time consuming due to the size of our dataset... 
        Perhaps there is a way to avoid fully re-evaluating the model? This is where the Recursive Least Squares (RLS) method comes into play, yielding the Recursive Autoregressive 
        model! This algorithm basically updates an older models coefficients by new data, without reevaluating the whole dataset. Let's get started with some intuitive math.

        <br><br>

        Let's focus on the formula for A and specifically, it's component \( X \). It is a matrix with size \( N \times p \), and as an example for a \( p=3 \), AR(3) process it contains 
        \( y(t-1), ..., y(t-p) \) in it's rows, starting from  \( t+p+1 \) to facilitate \( p \) lags. 

        \[
        X = \begin{bmatrix}
        y(4-1), ..., y(4-p) \\
        y(5-1), ..., y(5-p) \\
        \vdots \\
        y(N-1), ..., y(N-p)
      \end{bmatrix}
        \]

        Let's create an auxillary function to represent the multiplication \( (X^T X) \):

        \[
        u_i=[y(i-1),y(i-2),…,y(i-p)]
        \]

        with which we can write:

        \[
        F_N = X^T X = \sum_{i}^N = u_i u_i^T       
        \]

        \[
        X^T Y = \sum_i^N u_i y(i)
        \]

        with the appropriate initial \( i \). Now, manipulating the summation limit, we can write these elements as:

        \[
        \sum_i^{N+1} u_i u_i^T = \sum_i^N u_i u_i^T + u_{N+1} u_{N+1}^T
        \]

        Now, the above is essentially a \( F_{N+1} \) expression. <br><br>
        Since in the equation for our coefficients \(A\) includes the inverse of this multiplication, we will apply the 
        Matrix Inversion Lemma to it (if this sounds like much, it's just a proven formula for specific setups of
        matrix equations), and get the following: 

        \[
        F_{N+1}^{-1} = F_{N}^{-1} - \frac{
          F_{N}^{-1} u_{N+1} u_{N+1}^T  F_{N}^{-1}
        }{
          1 + u_{N+1}^T  F_{N}^{-1} u_{N+1}
        }
        \]

        Denoting part of the fraction as \( K \) and then rewriting the function with it:

        \[
        K_{N+1} = \frac{
          F_{N}^{-1} u_{N+1}
        }{
          1 + u_{N+1}^T  F_{N}^{-1} u_{N+1}
        }
        \]

        \[
        F_{N+1}^{-1} = F_{N}^{-1} - K_{N+1} u_{N+1}^T F_N^{-1}
        \]

        Which in turn, after some substitutions reveals a simpler formula for \( K \) itself, that we used to get previous formula:

        \[
        K_{N+1} = F_{N+1}^{-1} u_{N+1}
        \]

        Ok so now we need to figure out the component: \( X^T Y = \sum_i^N u_i y(i) \) - the second half of our coefficients expression. 
        <br><br>
        We will split it's sum to an \(N\) and \(N+1\) component, utilize the discovered equations for \(K\) and
        \(F\), do some algebra and arrive at a result that I hope will be useful in understanding the inner workings of the Kalman Filter, without having to derive it (it's much more tricky and tedious).
        <br><br>
        Consider we start from \(N+1\) samples, whittle  down from there. We can write \(A\) as:

        \[
        A_{N+1} = (X^T X)^{-1} X^T Y = F_{N+1}^{-1} \sum_i^{N+1} u_i y(i) = F_{N+1}^{-1} \biggl( \sum_i^{N} u_i y(i) + u_{N+1} y(N+1) \biggr)
        \]

        Substituting \(F_{N+1}^{-1} \) for it's previous iteration form and \( K \) summation:
        
        \[
        \biggl( F_{N}^{-1} - K_{N+1} u_{N+1}^T F_N^{-1} \biggr)\sum_i^{N} u_i y(i) + F_{N+1}^{-1} u_{N+1} y(N+1) 
        \]
        
        Opening up the brackets:

        \[
        \overbrace{ F_{N}^{-1} \sum_i^{N} u_i y(i) }^\text{optimal sol. at N}  - K_{N+1} u_{N+1}^T \overbrace{ F_N^{-1} \sum_i^{N} u_i y(i) }^\text{optimal sol. at N} + \overbrace{ F_{N+1}^{-1} u_{N+1} }^\text{K} y(N+1) 
        \]
        
        And with this, picking up the pieces into brackets and expressing K where possible, we arrive at a new, recursively updatable formula for A:

        \[
        A_{N+1} = A_N + K_{N+1} ( y(N+1) - u_{N+1}^T A_N )
        \]

        The component \( K \) is often in Kalman Filtering called the 'gain matrix', and it has different derivation paths, however the main goal of this component is the same in RLS as in
        Kalman Filtering - weight up how much to adjust the coefficients of regression based on how large the error between the new measurement and old model prediction is! NB: I'm talking
        about the Kalman Filter for the purpose of regression - something it's not really built for and most literature does not talk in terms relating to this.  
      </p>
      
      <h3>Kalman Filter:</h3>
      <p style="text-align:justify; "> 

        !NB: From here on, we start using the variable \(X\) as a coefficient holder in regression terms. Just a change in notation that kind off leaked through, so keep it in mind. <br><br>
        
        To start off, lets cover the basic application example of a Kalman Filter, introducing the concept state-space modelling in the process.<br>
        Consider a moving car and how we could know it's position. Well, we need some initial info, where the machine was before and how fast it's moving. We can also perhaps utilize
        GPS to find the cars position. Both methods will have some error. For estimation by past position and velocity we might make an error in those measurements, but also have the issue of estimating
        a continuous parameter in finite time, the offset of calculation time causes an error in the long term (more a theoretical issue, if we don't refresh our measurements at some point).
        For a GPS, errors arise from latency and instrument precision etc. A Kalman Filters goal in such a situation is to balance out the errors of both sources of information, to yield 
        a position estimate with higher accuracy than with either method alone.   

        Now, while the idea is relatively simple, quite specific issues arise in implementation. Namely, what do we compare either estimate to? What is the 'true' position? How are the 
        errors in either measurement different? What are the properties of these errors in terms of a stochastic process? 

        Lets write out a simple state-space equation for the problem of a moving car, and see what's what. First, the main system of equations:

        \[
        X = \begin{pmatrix} x \\ \dot{x} \end{pmatrix}  \ \ \ \ \  F = \begin{pmatrix} 1 & \Delta t\\ 0 & 1 \end{pmatrix} \ \ \ \ \ H = \begin{pmatrix} 1 \\ 0 \end{pmatrix}
        \]

        \[
        X_k = F X_{k-1} + w_k \]
        \[
        Z_k = H X_k + v_k
        \]

        One can multiply the matrix equations into a simpler form and notice that there is an equation for \(X\), which is simply the physical equations of motion, and then there's an equation for
        \(Z\), which is in fact the position attained from a GPS. X is called the 'state' of the system and can be estimated iteratively from previous states, by some equations connecting past to present.
        The \(Z\) equation is known as the 'measurement' equation, which states a relationship between the state estimate and the GPS estimate, their difference being governed by the noise 
        term \(v_k\). The latter does not help us 'estimate' a better value for the position directly, 
        but the relationship is what we build the Kalman Filter upon, specifically the Kalman Gain matrix \(K\) (similar to the RLS matrix \(K\)).

        The noise terms are assumed to be zero centered and with independent covariance matrixes:

        \[
        w_k \sim N(0,Q)
        \]
        \[
        v_k \sim N(0,R)
        \]
        
        These covariance matrixes must be estimated to actually make use of the Kalman Filter and the state-space equations. Generally, for conventional applications like the example here,
        \(Q\) is proportional to velocity and \(R\) might be derived based on the construction of the GPS system, however neither are known 100%. The value of this type of setup is that we can employ the Kalman Filter algorithm to recursively estimate a position for our vehicle, with noizy measurements from a GPS, by also considering 
        how a car previously at position 1 can get to position 2 at a given speed. All the information is there for us, we just use physics to improve GPS accuracy.

        <br><br>

        We can do the same 
        for regression problems, however with some trickery. The main idea is to swap the 'state' equation for one evolving regression coefficients in time, and the 'measurement' equation for 
        the regression model itself! Some difficulties arise due to us not having any good idea on how to pick \(R\) however, as we generally will be dealing with already measured data from less defined
        measurement device (not impossible, just in practice rarely the case). NB: we must choose an \(R\), as we cannot assume that our measured data is 100% accurate, otherwise the algorithm cannot balance 
        different noise sources. A second problem arises from choosing a state equation - while a car will move based on the laws of motion, how should regression coefficients evolve in time? 

        <br><br>

        These problems are not trivial, but generally simple solutions work well enough, e.g. evolving coefficients as a random walk process, thus \(Q\) defines a pseudo adaption-rate, 
        which becomes a tuning parameter. The covariance \(R\) represent then the coefficients suitability for modeling via regression a real measured value \(Z\). \(R\) in this setup is usually
        fixed as a constant or estimated from the data recursively. 

        <br><br>

      </p>
      <h4>Discrete Kalman Filter derivation:</h4>
      <p style="text-align:justify; "> 

        Let's start with a state-space equation, keeping the components abstract for now:
        
        \[
        X_{k+1} = A X_k + w_k \ \ \ \ \ \ \ \langle w_k w_k^T \rangle = Q
        \]
        \[
        Y_k = C X_k + v_k \ \ \ \ \ \ \ \langle v_k v_k^T \rangle = R
        \]

        Crucial to point out here, is that the matrix \(A\) modulates the propagation of the 'state'. As we intend to hide the regression coefficients in there,
        I point out that \(A\) will be chosen as a unit-matrix, essentially making the coefficients evolve unbiased. It can be a different matrix however, and is kept 
        in the formulas and derivation. <br> 
        Here we expand the notation a bit, using from now on \(X_{k|k}\) for the estimate of \(X\) at time \(k\), and \(X_{k+1|k}\) for the one step ahead estimate or evolution of the 
        state X. Now we can calculate for example the expected future value of the state:

        \[
        \langle X_{k+1|k} \rangle = A\langle X_{k|k} \rangle + \overbrace{w_k}^\text{ = 0} \rightarrow \hat{X}_{k+1|k} = A \hat{X}_{k|k}
        \]

        from which we can get the error/deviation to be expected between the estimate and real value of the state:

        \[
        X_{k+1|k} - \hat{X}_{k+1|k} = A X_{k|k} + w_k - (A \hat{X}_{k|k})
        \]

        and denoting \( \epsilon_{k+1|k} = X_{k+1|k} - \hat{X}_{k+1|k} \) we can write:

        \[
        \epsilon_{k+1|k} = A \epsilon_{k|k} + w_k
        \]

        Next up, we estimate a covariance for the state (before measurement is taken into account). That is, we are estimating the covariance of the state X, not it's driving process, 
        whose covariance is Q. We call this new covariance \(P\) and calculate it as:

        \[
        P_{k+1|k} = \langle \epsilon_{k+1|k} \epsilon_{k+1|k}^T \rangle = \langle [ X_{k+1|k} - \hat{X}_{k+1|k} ] [ X_{k+1|k} - \hat{X}_{k+1|k} ]^T \rangle \rightarrow 
        \]
        \[
        \rightarrow P_{k+1|k} = A \langle \epsilon_{k|k} \epsilon_{k|k}^T \rangle A^T + \overbrace{\langle w_k w_k^T \rangle}^\text{ = Q} \rightarrow 
        \]
        \[
        \rightarrow P_{k+1|k} = A P_{k|k} A^T + Q 
        \]

        Now we need to update this state covariance based on the discrepancy between the state-based estimate of the time-series value and the measurement (the actual value we have).
        That's to say, based on the error term making up the noise process \(v_t\). To do this, we need to model the measurement too however, as we cannot assume it to be immutable and 100% accurate. 

        \[
        Y_{k+1} = C X_{k+1|k} + \overbrace{v_{k+1}}^{ \langle v_{k+1} \rangle = 0  } \rightarrow \langle Y_{k+1} \rangle = \hat{Y}_{k+1} = C \hat{X}_{k+1|k}
        \]
        \[
        Y_{k+1} - \hat{Y}_{k+1} = C X_{k+1|k} + v_{k+1} - (C \hat{X}_{k+1|k}) 
        \]
        \[
        \epsilon_{Y;k+1} = C \epsilon_{k+1|k} + v_{k+1}
        \]

        The notation of \( \epsilon_{Y;k+1} \) is a bit cumbersome, but notice the \(Y\) in there before the time step notation. This essentially gives us a link between the covariance of the state
        and measurement. Moving on, we need to use this knowledge to update the state, based on the incoming measurement. We do this by proposing (in this derivation) an ansatz or educated guess
        that we can do this similarly to the RLS algorithm:

        \[
        \hat{X}_{k+1|k+1} = \hat{X}_{k+1|k} + K_{k+1} ( Y_{k+1} - [ C \hat{X}_{k+1|k} ] )
        \]

        where \( K \) is a gain that adapts the new error to the previous estimates. We need to find it's shape however - it's not the same as in RLS. We can do that by some further 
        algebra and with a bit of derivatives:

        \[
        \hat{X}_{k+1|k+1} = \hat{X}_{k+1|k} + K_{k+1} \epsilon_{Y;k+1}
        \]
        \[
        \hat{X}_{k+1|k+1} = \hat{X}_{k+1|k} + K_{k+1}  C \epsilon_{k+1|k} + K_{k+1} v_{k+1}
        \]

        The last line tells us how the state will be updated based on the error between the actual measurement and the expected measurement (based on the former state), and as we wish this error
        to be minimal, we can do that by choosing the right \(K\). NB: we want to optimize \(K\) based on the state, i.e. tho have the state approach a hypothetical "true" state as closely as possible, 
        thus we employ it's covariance matrix \(P\) and seek to have it's derivative with respect to \(K\) to be zero. Let's construct a new \( \epsilon_{k|k} \) for the Gain influenced formula, then the respective 
        \(P\), then take the derivative and see where we end up.

        \[
        X_{k+1} - \hat{X}_{k+1|k+1} = X_{k+1} - \hat{X}_{k+1|k} - K_{k+1} C \epsilon_{k+1|k} - K_{k+1}v_{k+1}
        \]
        \[
        \epsilon_{k+1|k+1} = (I - K_{k+1}C)\epsilon_{k+1,k} - K_{k+1}v_{k+1}
        \]

        The covariance is then:

        \[
        P_{k+1|k+1} = \langle \epsilon_{k+1|k+1} \epsilon_{k+1|k+1}^T \rangle
        \]

        which after some algebra yields a relationship with it's previous values:
        
        \[
        P_{k+1|k+1} = (I - K_{k+1}C)P_{k+1|k}(I-K_{k+1}C)^T + K_{k+1} R K_{k+1}^T 
        \]

        Taking a partial derivative of the trace of the above (the trace of the covariance are the errors, however operating on the covariance matrix is necessary to access them all in a VAR context), 
        with respect to \(K\) (requires some calculation, consider trace operator properties in relation to derivatives, skipped here to result):

        \[
        \frac{\delta}{\delta K_{k+1}}\text{tr}[P_{k+1|k+1}] = 0 - (2 C P_{k+1|k})^T + 2 K_{k+1} (C P_{k+1|k}C^T + R)
        \]

        which yields for \(K\) the following expression:

        \[
        K_{k+1} = P_{k+1|k} C^T (C P_{k+1|k}C^T + R)^{-1}
        \]

        NB: there's some position swapping taking place here due to the transposes. Also, the component with the inverse brackets around it is often noted in literature as \(S\). 

        If we now substitute this \(K\) into the formula for \(P_{k+1|k+1}\), we get our final update equation (after which we will also list the steps involved in utilizing them):

        \[
        P_{k+1|k+1} = (I - K_{k+1}C)P_{k+1|k}
        \]

        How do we actually use these equations now? Well, we start with an initial estimate or best guess for \(X\), then calculate the error between measurement and state-based estimate, 
        calculate the Kalman Gain, update the state \(X\) with the Gain, update the state covariance similarly, propagate the state forward (through the covariance matrix, the coefficients are developed only 
        by the Kalman Gain, not actual noise addition - the latter is important for deriving proper equations, but it's mean is zero) and repeat!

        Putting the operations in order, we have the Kalman Filter algorithm as a list of steps: 

        <br>
        State evolution:

        \[
        \hat{X}_{k+1|k} = A \hat{X}_{k|k} + w_k
        \]
        \[
        P_{k+1|k} = A P_{k|k}A^T + Q
        \]

        <br>
        Update step:

        \[
        \epsilon_{Y;k+1} = Y_{k+1} - C \hat{X}_{k+1|k}
        \]
        \[
        S_{k+1} = (C P_{k+1|k}C^T + R)
        \]
        \[
        K_{k+1} = P_{k+1|k} C^T S_{k+1}^{-1}
        \]
        \[
        \hat{X}_{k+1|k+1} = \hat{X}_{k+1|k} + K_{k+1}\epsilon_{Y;k+1}
        \]
        \[
        P_{k+1|k+1} = (I - K_{k+1} C) P_{k+1|k}
        \]

        and rinse-and-repeat! This is all well and good, but how to actually use the above to perform Kalman Filtered adaptive regression? For this, we need to bring in some clever 
        variables, that can fit the regression coefficients into the state \(X\).

      </p>
      <h4>Kalman Filter setup for regression:</h4>
      <p style="text-align:justify; "> 

        Let's finalize the Kalman Filter for regression. To do so, we restate the model we wish to solve:
        
        \[
          y_t = \sum_{\tau=1}^{p} c_\tau(t) \ y_{t-\tau} + \epsilon(t)
        \]
        
        As this is the same equation for an AR process that we started with (albeit with time dependence denoted as an lower index), just the coefficients now evolve in time.
        Here, \(y_t\) can be considered a vector of time-series values at time \(t\), but for simplicity of notation I'll keep the lowercase letter. The same is true for the 
        coefficients \(c_{\tau}\), which inside the summation are still vectors - one for each equation, from all other equations, at the lag \(\tau\). 
        
        \[
        c_{\tau}(t) = \begin{bmatrix}
        c_{11}^{\tau}(t) c_{12}^{\tau}(t) ... c_{1D}^{\tau}(t) \\
        c_{21}^{\tau}(t) c_{22}^{\tau}(t) ... c_{2D}^{\tau}(t) \\
        \vdots \\
        c_{D1}^{\tau}(t) c_{D2}^{\tau}(t) ... c_{DD}^{\tau}(t)
      \end{bmatrix}
      \ \ \ \ \ \ 
      
      y_t = \begin{bmatrix}
      y_{1,t} \\
      y_{2,t} \\
      \vdots \\
      y_{D,t}
      \end{bmatrix}

      \]
        
        with \(D\) being the dimensionality or number of time-series in the VAR model we are solving. These example coefficients apply for the first time-series, as the index \(ij\) represents
        the regression coefficient in the direction \( j \rightarrow i \). <br> 

        We could format this entirely as a matrix multiplication as well, and we shall - but for this we need to include some additional matrix stacking tools. Let's get right to it! <br><br> 
                
        To estimate such coefficients, we formulate a state-space model with the following components: 

        \[
        \hat{X}_t = vec([c_1(t), ..., c_p(t)]^T) \ \ \ \ \ Y_t = ( y_{t}^T, y_{t-1}^T, ...,  y_{t-p+1}^T ) \ \ \ \ \ C_t = I_d \otimes Y_t^T
        \]

        where \(vec\) is the vectorization operator (stacking a matrixes rows into a vector), and \(\otimes\) is the Kronecker product, both serving as a matrix stacking 
        operators, allowing VAR models to be estimated (multiplying out an example will yield \(D\) equations of order \(p\), with \(D\) being the number of time-series in the VAR). 
        The model is then written as: 

        \[
        \hat{X}_{t+1} = \hat{X}_t + v_t
        \]
        \[
        y_{t} = C_{t-1}^T \hat{X}_t + \epsilon_t
        \]

        with

        \[
        Cov(\epsilon_t)=R_t
        \]
        \[
        Cov(v_t)=Q_t
        \]

        This model is then estimated through the standard Kalman Filter stepwise algorithm. <br>
        First, the 'estimation step':

        \[
        \hat{X}_{t+1|t} = \hat{X}_{t|t}
        \]

        \[
        P_{t+1|t} = P_{t|t} + Q_t
        \]

        Here the lower indexes \( t|t \) and \( t+1|t \) represent the values before and after iterating through the state equation respectively (or after and before the measurement step, respectively).
        Just a reminder - this implies an infinite recursion, which is true, but in practice we just fix an initial guess and start from there. <br>
        Next comes the 'Measurement update' step:

        \[
        e_{t+1} = y_{t+1} - C_t^T \hat{X}_{t+1|t}
        \]
        \[
        S_{t+1} = ( C_t^T P_{t+1|t} C_t + R_t )
        \]
        \[
        G_{t+1} = P_{t+1|t} C_t S^{-1}_{t+1} 
        \]
        \[
        \hat{X}_{t+1|t+1} = \hat{X}_{t+1|t} + G_{t+1} \epsilon_{t+1}
        \]
        \[
        P_{t+1|t+1} = (I - G_{t+1}C_t^T)P_{t+1|t}
        \]

        at which point \( t+1|t+1 \) becomes \( t|t \) and we move to the next measurement in the loop. 
        
      </p>
      <h4>Rudimentary Python implementation:</h4>
      <p style="text-align:justify; "> 

        I've build my implementation to operate on data in the form of a pandas dataframe, with columns representing time indexes, and rows representing distinct time-series - 
        the measurement dimensions. Considering \(D\) to be the dimensionality, or the number of time-series (with length \(N\) steps) analyzied in the VAR model, and \(p\) the regression order, then 
        the code yields a \(N-p\) coefficient matrixes. The latter are structured as follows:

        \[
        [c_{11}(t-1), c_{12}(t-1), ..., c_{1D}(t-1), c_{11}(t-2), ..., c_{1D}(t-p), c_{21}(t-1),...,c_{2D}(t-1),...c_{DD}(t-p)]
        \]
        
        
        The above list of coefficients in this specific order, multiplied by the vectorized \(p\) past values of the measurement vector that has been Kronecker multiplied in turn with 
        a \(D\text{x}D\) identity matrix yields exactly \(D\) equations with all dimensional directions summed up with their respective coefficients. The object 'Xhatstorage' in the below code
        contains a list of such coefficient matrixes, each element in the list corresponding to a time step. <br><br>
        !NB: The \(c_{ij}\) format used here is the same as explained above before the vectorization operation - this is the result of that procedure! <br>
        !NB2: the notation \(\hat{X}\) is represented as "Xhat" in the code, with "Xhatminus" beign the prior and "Xhatplus" the posterior, or \(\hat{X}_{k|k}\) and \(\hat{X}_{k+1|k}\) respectively
        (some sources simply note the indexes not by \(k|k\) and \(k+1|k\), but \(k-1|k\) and \(k|k\) respectively, and I've lost track of which source I used at which point of development). 

        <br><br>

        I've included comments in the code to certain scientific paper that discuss an implementation such as this - their citations are given below the snippet. 


        <pre class="line-numbers">
          <code class="language-css">
            def kalman_var(data_in, order, lamda = 0.03): #The lambda parameter is set to 0.03 by default, since that is what many use, citing efficiency (see Milde 2010)

              import numpy as np
              
              data = data_in.values #making the data into a numpy array for comfortable manipulation
              lam = lamda 
              #lambda is a sort of adaptation constant, governing the change of measurement noize, which in our-case is pseudonoise of the TS process itself
              
              p = order #VAR model order
              
              m = data.shape[0] 
              #Checking how many rows in data input = dimensions or number of time-series
              
              sL = data.shape[1] 
              #Checking length of time-series, "sL" as in series-length
              
              unit=p*m
              tab = np.zeros([m, m], dtype=object)
              for i in range(1,m+1):
                  for j in range(1,m+1):
                      temp_tab = []
                      for add in range(0,p):
                          temp_tab.append(
                              ((unit*(i-1))+(j-1))+(m*add)
                          )
                      tab[j-1,i-1] = temp_tab # COLUMNS means TO WHERE, row means FROM WHERE 

              Xhatminus = [0]*(p*m**2)
              
              #Below making a Xhatminus that is essentially the naive prediction for the first step (average of own values til p of past steps)
              #Can be much simpler, but this seems to aid the model in converging quicker
              for i in range(len(tab)):
                  for j in range(len(tab[i])):
                      for coef_i in tab[i][j]:
                          if i == j:
                              Xhatminus[coef_i] = (1/p)
                          else:
                              Xhatminus[coef_i] = 0

              F = np.identity((m**2)*p) 
              #F is the transition matrix, when it is a unit matrix it indicates that the coefficients evolve as a random walk. 
              
              Q = np.identity((m**2)*p)*(10**(-2)) 
              #Q is the state process noise covariance matrix. The states evolve independently, so unity matrix. 
              #Multiplied by a small number to indicate some noize in their initial evolution
              
              E = np.array([1]*m) 
              #Just initializing the error holder or in fact, the measurement process noize values. 
              #This is not used and calculated later as series_value - predicted_value 
              
              R = np.identity(m) 
              #Initializing covariance of data noize through data itself, values up to 10. 
              #Should then start analysis from 10th time point, but for testing its ok. 
              
              Pmin = np.identity((m**2)*p)  
              #The initial one step ahead prediction of the state covariance. 
              
              Yhat = np.zeros((m,(sL))) 
              #Creating a prediction array, that will contain predicted values based on the state estimates.

              Xhatstorage = [Xhatminus]*p
              Errorstorage = [[0]*m]*p
              Rstorage = [np.eye(m)]*p #[[[1, 1],[1, 1]]]*p
              Pstorage = [np.identity((m**2)*p)]*p

              for i in range(p,sL): #This is the Kalman VAR recursion, based on Havlicek 2010 and Schlögl 2000 (specifically the adaption criteria lambda etc)
                  
                  Y = data[:,range((i-p),i+1)] 
                  #Partitioning necessary data for p-length loop, including next one for update and prediction comparison.

                  H = np.kron( np.identity(m) , Y[:, range(0,p)].transpose().flatten() )

                  K = np.dot( np.dot(Pmin, H.transpose()) , np.linalg.inv( np.dot( np.dot( H, Pmin ), H.transpose() ) + R ) ) 
                  #Calculating the Kalman Gain

                  d  = Y[:,p] - np.dot(H, Xhatminus)

                  Xhatplus = np.dot(F , ( Xhatminus + np.dot(K ,  d) ) ) 
                  #Calculating new coefficients

                  E = Y[:,p] - np.dot(H, Xhatplus) 
                  #Calculating error from estimate and actual time-series noise

                  R2 = lam*R + (1-lam)*( np.dot([[item] for item in E],[E]) + np.dot(np.dot(H,Pmin),np.transpose(H)) )
                  R = R2

                  P = Pmin - np.dot( np.dot(K , H) , Pmin ) 
                  #Calculating the estimate error covariance - this is essentially the estimate of the error from the REAL coefficients, 
                  #we are trying to find - separate from state evolution noise. 

                  Q2 = lam*Q + (1-lam)*( np.dot(np.dot(K, np.dot([[item] for item in d],[d]) ),np.transpose(K)) )
                  Q = Q2

                  Pplus = ( np.dot( np.dot(F, P) , F) ) + Q 
                  #Updating the estimate error covariance

                  Pmin = Pplus

                  Errorstorage.append(E)

                  Xhatstorage.append(Xhatplus) 
                  #storing loop values, moving on after that

                  Rstorage.append(R)
                  Xhatminus = Xhatplus
                  Pstorage.append(Pplus)

                  for h in range(0,m):
                      Yhat[h,i] = np.dot(H[h], Xhatplus)

              return Yhat, Xhatstorage, Errorstorage, Rstorage, Pstorage


          </code>
        </pre>

        Relevant citations:
        <br>
        1) Milde 2010, doi: 10.1016/j.neuroimage.2009.12.110. Epub 2010 Jan 7. <br>
        2) Havlicek 2010,  doi: 10.1016/j.neuroimage.2010.05.063. Epub 2010 Jun 1. <br>
        3) Schlögl 2000, doi: 10.1109/IEMBS.2000.898046 <br>
        4) Akhlaghi 2017, doi: 10.1109/PESGM.2017.8273755 <br>
        5) Oya 2007, doi: 10.1016/j.biosystems.2006.05.018 <br>

        <br><br>
        <br><br>

      </p>

      <p> 
        Having gotten to the end, I stress again that the effectiveness of the extracted regression coefficients depend on proper parameter tuning, initial values and data 
        preprocessing. The algorithm most certainly needs quite a bit more work, mainly in optimization and better selection of R, Q, and initial parameters. 
        My aim with this summary post is to provide a holistic overview for amateurs in dealing with Kalman Filters, who wish to try their hand in implementation, or are simply curious about 
        the intuitive workings of such algorithms. <br> 
        
        Other algorithms I've worked on in relation to the Kalman Filter VAR can be found in my Github under the <a href=https://github.com/SanderPaekivi/KalmanFilter-VectorAutoregression- style=”display:none”>"KalmanFilter-VectorAutoregression"</a> repo. 
        
        <br><br>

        I hope this overview has been helpful and wish you the best of luck in atempting to wrangle the Kalman Filter! 
      </p>


          



</div>



<footer>
  <br>
  <br>
  <br>
</footer>

